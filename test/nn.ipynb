{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of spectrogram 0: torch.Size([1, 129, 101])\n",
      "Shape of spectrogram 1: torch.Size([1, 129, 101])\n",
      "Shape of spectrogram 2: torch.Size([1, 129, 101])\n",
      "Shape of spectrogram 3: torch.Size([1, 129, 101])\n",
      "Shape of spectrogram 4: torch.Size([1, 129, 101])\n",
      "Shape of spectrogram 5: torch.Size([1, 129, 101])\n",
      "Shape of spectrogram 6: torch.Size([1, 129, 101])\n",
      "Shape of spectrogram 7: torch.Size([1, 129, 101])\n",
      "Shape of spectrogram 8: torch.Size([1, 129, 101])\n",
      "Shape of spectrogram 9: torch.Size([1, 129, 101])\n",
      "Shape of spectrogram 10: torch.Size([1, 129, 101])\n",
      "Shape of spectrogram 11: torch.Size([1, 129, 101])\n",
      "Shape of spectrogram 12: torch.Size([1, 129, 101])\n",
      "Shape of spectrogram 13: torch.Size([1, 129, 101])\n",
      "Shape of spectrogram 14: torch.Size([1, 129, 101])\n",
      "Shape of spectrogram 15: torch.Size([1, 129, 101])\n",
      "Shape of spectrogram 16: torch.Size([1, 129, 101])\n",
      "Shape of spectrogram 17: torch.Size([1, 129, 101])\n",
      "Shape of spectrogram 18: torch.Size([1, 129, 101])\n",
      "Shape of spectrogram 19: torch.Size([1, 129, 101])\n",
      "Shape of spectrogram 20: torch.Size([1, 129, 101])\n",
      "Shape of spectrogram 21: torch.Size([1, 129, 101])\n",
      "Shape of spectrogram 22: torch.Size([1, 129, 101])\n",
      "Shape of spectrogram 23: torch.Size([1, 129, 101])\n",
      "Shape of spectrogram 24: torch.Size([1, 129, 101])\n",
      "Shape of spectrogram 25: torch.Size([1, 129, 101])\n",
      "Shape of spectrogram 26: torch.Size([1, 129, 101])\n",
      "Shape of spectrogram 27: torch.Size([1, 129, 101])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torch.nn.functional as F  # Import F for padding\n",
    "\n",
    "# Path to the directory containing the saved NumPy arrays\n",
    "NUMPY_DIR = \"keystroke_spectrograms/numpy_arrays\"\n",
    "\n",
    "# Function to load all spectrograms from the directory\n",
    "def load_spectrograms_from_directory(directory):\n",
    "    spectrograms = []\n",
    "    keys = []\n",
    "    widths = []\n",
    "    filenames = os.listdir(directory)\n",
    "\n",
    "    # Loop through each file in the directory\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.npy'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "\n",
    "            # Load the NumPy array from file\n",
    "            spectrogram = np.load(file_path)\n",
    "\n",
    "            widths.append(spectrogram.shape[1])\n",
    "\n",
    "            key = re.search(r\"keystroke_\\d+_([A-Za-z])\\.npy\", filename).group(1)\n",
    "\n",
    "            # Convert the spectrogram to a PyTorch tensor\n",
    "            # Adding a channel dimension (1, height, width) for CNN input\n",
    "            spectrogram_tensor = torch.tensor(spectrogram).unsqueeze(0).float()  # Shape: (1, n_freq_bins, n_time_bins)\n",
    "\n",
    "            # next, add the tensor to a list\n",
    "            spectrograms.append(spectrogram_tensor)\n",
    "            keys.append(key)\n",
    "\n",
    "    return spectrograms, keys, max(widths)\n",
    "\n",
    "# Load all spectrograms from the specified directory\n",
    "spectrogram_tensors, keys, max_width = load_spectrograms_from_directory(NUMPY_DIR)\n",
    "\n",
    "assert len(spectrogram_tensors) == len(keys), \"The number of spectrograms and keys do not match!\"\n",
    "\n",
    "\n",
    "## Need to pad the spectrograms to the same width\n",
    "\n",
    "# Padding function\n",
    "def pad_tensor(tensor, max_width):\n",
    "    # Calculate padding size (for the width dimension)\n",
    "    current_width = tensor.shape[2]  # Index 2 is for the time dimension (width)\n",
    "    if current_width < max_width:\n",
    "        padding = max_width - current_width\n",
    "        # Pad with zeros on the right (width dimension)\n",
    "        return F.pad(tensor, (0, padding))  # Pad (left, right)\n",
    "    return tensor\n",
    "\n",
    "# Apply padding to all spectrograms\n",
    "for i in range(len(spectrogram_tensors)):\n",
    "    spectrogram_tensors[i] = pad_tensor(spectrogram_tensors[i], max_width)\n",
    "\n",
    "# Verify padding\n",
    "for i, spectrogram in enumerate(spectrogram_tensors):\n",
    "    print(f\"Shape of spectrogram {i}: {spectrogram.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeystrokeDataset(Dataset):\n",
    "    def __init__(self, spectrograms_tensors, labels):\n",
    "        self.spectrograms = spectrograms_tensors\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.spectrograms)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.spectrograms[idx], self.labels[idx]\n",
    "    \n",
    "tensors,labels, _ = load_spectrograms_from_directory(NUMPY_DIR)\n",
    "tensors = spectrogram_tensors # adjusting the tensors to the padded spectrograms\n",
    "train_dataset = KeystrokeDataset(spectrogram_tensors,labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "#test_dataset = KeystrokeDataset(test_spectrograms_tensors, test_labels)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeystrokeCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KeystrokeCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # 1 input channel (grayscale)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 7 * 7, 512)  # Adjust size based on input dimensions\n",
    "        self.fc2 = nn.Linear(512, 26)  # 26 output classes (A-Z)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass through convolutional layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # Flatten the output from convolutional layers to feed into fully connected layers\n",
    "        x = x.view(-1, 128 * 7 * 7)  # Adjust based on input size (H, W)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = KeystrokeCNN()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 6272]' is invalid for input of size 688128",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m                 running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[30], line 24\u001b[0m, in \u001b[0;36mKeystrokeCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool2d(x, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Flatten the output from convolutional layers to feed into fully connected layers\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust based on input size (H, W)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Fully connected layers\u001b[39;00m\n\u001b[1;32m     27\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 6272]' is invalid for input of size 688128"
     ]
    }
   ],
   "source": [
    "# Function to train the model\n",
    "def train(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            if i % 200 == 199:  # Print every 200 batches\n",
    "                print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 200:.3f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "# Train the model\n",
    "train(model, train_loader, criterion, optimizer, epochs=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
