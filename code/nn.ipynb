{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if .gitignore exists and handle *.npy exception\n",
    "\n",
    "try:\n",
    "    os.chdir('/home/test')\n",
    "    os.listdir()\n",
    "except:\n",
    "    print(\"Could not change directory to /home/test\")\n",
    "    print(\"If you intended to run the code locally, you can skip this message\")\n",
    "\n",
    "gitignore_path = \"../.gitignore\"\n",
    "if os.path.exists(gitignore_path):\n",
    "    # Read current content\n",
    "    with open(gitignore_path, \"r\") as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Only append if *.npy is not already in the file\n",
    "    if \"*.npy\" not in content:\n",
    "        with open(gitignore_path, \"a\") as f:\n",
    "            f.write(\"\\n*.npy\")  # Add newline before appending\n",
    "else:\n",
    "    # Create new .gitignore if it doesn't exist\n",
    "    with open(gitignore_path, \"w\") as f:\n",
    "        f.write(\"*.npy\")\n",
    "\n",
    "\n",
    "DATA_DIR = input(\"Enter the name of the directory\")\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    print(f\"Error: The directory {DATA_DIR} does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not extract key from filename: keystroke_67_Key.esc.npy. Make sure it is a valid key (aA-zZ)\n",
      "loaded 66 spectrograms\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torch.nn.functional as F  # Import F for padding\n",
    "\n",
    "# Path to the directory containing the saved NumPy arrays\n",
    "DATA_DIR = \"dell_test\"\n",
    "NUMPY_DIR = DATA_DIR + \"/numpy_arrays\"\n",
    "BASE_SIZE = torch.Size([1,129,300])\n",
    "\n",
    "# Padding function\n",
    "def pad_tensor(tensor, target_width):\n",
    "    current_width = tensor.shape[2]\n",
    "    if current_width < target_width:\n",
    "        padding = target_width - current_width\n",
    "        return F.pad(tensor, (0, padding))  # Pad the time dimension\n",
    "    elif current_width > target_width:\n",
    "        return tensor[:, :, :target_width]  # Truncate to target width\n",
    "    return tensor\n",
    "\n",
    "# Function to load all spectrograms from the directory\n",
    "def load_spectrograms_from_directory(directory):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    spectrograms = []\n",
    "    keys = []\n",
    "    widths = []\n",
    "    filenames = os.listdir(directory)\n",
    "\n",
    "    # Loop through each file in the directory\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.npy'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "\n",
    "            # Load the NumPy array from file\n",
    "            spectrogram = np.load(file_path)\n",
    "\n",
    "            widths.append(spectrogram.shape[1])\n",
    "\n",
    "            try: \n",
    "                key = re.search(r\"keystroke_\\d+_([A-Za-z])\\.npy\", filename).group(1)\n",
    "            except AttributeError:\n",
    "                print(f\"Error: Could not extract key from filename: {filename}. Make sure it is a valid key (aA-zZ)\")\n",
    "                continue\n",
    "\n",
    "            # Convert the spectrogram to a PyTorch tensor\n",
    "            spectrogram_tensor = torch.tensor(spectrogram).float().to(device)\n",
    "\n",
    "            \n",
    "\n",
    "            if spectrogram_tensor.shape != BASE_SIZE:\n",
    "                print(f\"Warning : Spectrogram tensor shape {spectrogram_tensor.shape} does not match the expected size {BASE_SIZE}\")\n",
    "                continue\n",
    "            \n",
    "            #spectrogram_tensor = spectrogram_tensor.unsqueeze(0)\n",
    "\n",
    "            \n",
    "            # Rearrange dimensions from [80, 13, 4] to [4, 80, 13]\n",
    "            #spectrogram_tensor = spectrogram_tensor.permute(2, 0, 1)\n",
    "            \n",
    "            # Pad or truncate the time dimension to match BASE_SIZE[2] (300)\n",
    "            \"\"\"\"\n",
    "            if spectrogram_tensor.shape[2] < BASE_SIZE[2]:\n",
    "                # Pad with zeros\n",
    "                padding = BASE_SIZE[2] - spectrogram_tensor.shape[2]\n",
    "                spectrogram_tensor = F.pad(spectrogram_tensor, (0, padding))\n",
    "            else:\n",
    "                # Truncate to desired length\n",
    "                spectrogram_tensor = spectrogram_tensor[:, :, :BASE_SIZE[2]]\n",
    "\n",
    "            if spectrogram_tensor.shape != BASE_SIZE:\n",
    "                print(f\"Warning: Spectrogram tensor shape {spectrogram_tensor.shape} does not match the expected size {BASE_SIZE}\")\n",
    "            \"\"\"\n",
    "\n",
    "            # Add the tensor to the list\n",
    "            spectrograms.append(spectrogram_tensor)\n",
    "            keys.append(key)\n",
    "\n",
    "    print(f\"loaded {len(spectrograms)} spectrograms\")\n",
    "    return spectrograms, keys, max(widths)\n",
    "\n",
    "\n",
    "spectrogram_tensors, keys, max_width = load_spectrograms_from_directory(NUMPY_DIR)\n",
    "\n",
    "assert len(spectrogram_tensors) == len(keys), \"The number of spectrograms and keys do not match!\"\n",
    "\n",
    "\n",
    "## Need to pad the spectrograms to the same width\n",
    "\n",
    "\n",
    "# Apply padding to all spectrograms\n",
    "\"\"\"\n",
    "for i in range(len(spectrogram_tensors)):\n",
    "        if (padded := pad_tensor(spectrogram_tensors[i], 300)) is not None:\n",
    "            spectrogram_tensors[i] = padded\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Verify padding\n",
    "for i, spectrogram in enumerate(spectrogram_tensors):\n",
    "    #print(f\"Shape of spectrogram {i}: {spectrogram.shape}\")\n",
    "    pass\n",
    "\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "char2idx = {char: idx for idx, char in enumerate(alphabet)}\n",
    "label_indices = [char2idx[char] for char in keys]\n",
    "label_tensor = torch.tensor(label_indices, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeystrokeDataset(Dataset):\n",
    "    def __init__(self, spectrograms_tensors, labels):\n",
    "        self.spectrograms = spectrograms_tensors\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.spectrograms)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.spectrograms[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional : save the tensors in a file for storing\n",
    "* the following cell is optional and can be skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paramiko\n",
    "from scp import SCPClient\n",
    "\n",
    "def export_data():\n",
    "\n",
    "    keyboards = (\"dell\",\"dell_test\",\"macbook\")\n",
    "\n",
    "    hostname = \"109.222.56.181\"\n",
    "    port = 5002\n",
    "    username = \"test\"\n",
    "    password = \"test\"\n",
    "\n",
    "    client = paramiko.SSHClient()\n",
    "    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "\n",
    "    for keyboard in keyboards: \n",
    "        try:\n",
    "            try : \n",
    "                client.connect(hostname, username=username, password=password,timeout=20,port=port)\n",
    "            except TimeoutError:\n",
    "                print(\"could not establish a connection\")\n",
    "\n",
    "            scp = SCPClient(client.get_transport())\n",
    "            scp.put(f\"{keyboard}/numpy_arrays\", f\"/home/test/{keyboard}\", recursive=True)\n",
    "            print(\"migrated the numpy arrays\")\n",
    "            scp.put(f\"{keyboard}/aligned_iphone.wav\", f\"/home/test/{keyboard}\")\n",
    "            print(\"migrated the audio data\")\n",
    "            scp.put(f\"{keyboard}/key_log.csv\", f\"/home/test/{keyboard}\")\n",
    "            print(\"migrated the keyboard keystroke data\")\n",
    "\n",
    "        finally:\n",
    "            client.close()\n",
    "\n",
    "    # export the data to GPU server\n",
    "    #from subprocess import run\n",
    "    #run(['sshpass', '-p', 'test', 'scp', '-r', '-P', '5002', '-o', 'StrictHostKeyChecking=no', '-o', 'UserKnownHostsFile=/dev/null', '/Users/oscartesniere/Documents/GitHub/MAIS202-FinalProject/code/', 'test@oscartesniere.com:/home/test'])\n",
    "\n",
    "export_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not change directory to /home/test\n",
      "If you intended to run the code locally, you can skip this message\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:\n",
    "    os.chdir('/home/test')\n",
    "    loaded_data = torch.load('keystroke_data.pt')\n",
    "    # Extract the tensors and labels\n",
    "    spectrogram_tensors = loaded_data['tensors']\n",
    "    label_tensor = loaded_data['labels']\n",
    "except:\n",
    "    print(\"Could not change directory to /home/test\")\n",
    "    print(\"If you intended to run the code locally, you can skip this message\")\n",
    "    \n",
    "\n",
    "train_dataset = KeystrokeDataset(spectrogram_tensors,label_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeystrokeCNN(nn.Module):\n",
    "    def __init__(self,input_height=129, input_width=300, num_classes=26,input_channels=1):\n",
    "        super(KeystrokeCNN, self).__init__()\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, padding=1)  # (1, H, W) -> (32, H, W)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) # (32, H, W) -> (64, H, W)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # (64, H, W) -> (128, H, W)\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # Reduces dimensions by half (H/2, W/2)\n",
    "\n",
    "        # Calculate the final feature map size dynamically\n",
    "        self._to_linear = self._get_conv_output_size(input_height, input_width)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512)  \n",
    "        self.fc2 = nn.Linear(512, num_classes)  # 26 output classes (A-Z)\n",
    "        #list(string.ascii_lowercase) for generating the output classes\n",
    "\n",
    "    def _get_conv_output_size(self, height, width):\n",
    "        \"\"\"Pass a dummy tensor to determine final feature map size after convolutions\"\"\"\n",
    "        x = torch.zeros(1, self.input_channels, height, width)  # Batch size = 1, 1 channel, (H, W)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        return x.numel()  # Flattened size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        print(\"Input shape:\", x.shape)\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "       # print(\"After conv1+pool:\", x.shape)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "       # print(\"After conv2+pool:\", x.shape)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "       # print(\"After conv3+pool:\", x.shape)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print(\"After flattening:\", x.shape)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = KeystrokeCNN()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeystrokeCNNv2(nn.Module):\n",
    "    def __init__(self, input_height=80, input_width=300, num_classes=26, input_channels=4):\n",
    "        super(KeystrokeCNNv2, self).__init__()\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        \n",
    "        # 3D Convolutional layers\n",
    "        self.conv1 = nn.Conv3d(input_channels, 32, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.conv2 = nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.conv3 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "\n",
    "        # 3D Pooling layer - only pool spatial dimensions, not channels\n",
    "        self.pool = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
    "\n",
    "        # Calculate the final feature map size dynamically\n",
    "        self._to_linear = self._get_conv_output_size(input_height, input_width)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _get_conv_output_size(self, height, width):\n",
    "        x = torch.zeros(1, 1, self.input_channels, height, width)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        return x.numel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape input to 5D tensor\n",
    "        x = x.unsqueeze(1)  # Add channel dimension for 3D conv\n",
    "        \n",
    "        # 3D convolutions with ReLU and pooling\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "#model = KeystrokeCNNv2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "lr = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr)\n",
    "\n",
    "#keyboard_names = ('dell', 'macbook', 'lenovo') # enter any new keyboard here for identification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\nfor layer in conv_layers:\\n    # Get weights for this specific layer\\n    weights = layer.weight.data  # Shape: (out_channels, in_channels, kernel_size, kernel_size)\\n\\n    # Calculate grid size based on number of output channels\\n    n_filters = layer.out_channels\\n    n_cols = 8  # Fixed number of columns\\n    n_rows = (n_filters + n_cols - 1) // n_cols  # Calculate rows needed\\n\\n    # Create figure with appropriate size\\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 2*n_rows))\\n    axes = axes.ravel()\\n\\n    # Plot each filter\\n    for i in range(n_filters):\\n        # Get the first channel of the filter (since we want to visualize 2D)\\n        filter_weights = weights[i, 0].cpu().numpy()\\n\\n        # Plot the filter\\n        axes[i].imshow(filter_weights, cmap=\\'gray\\')\\n        axes[i].axis(\\'off\\')\\n        axes[i].set_title(f\\'Filter {i+1}\\')\\n\\n    # Hide empty subplots if any\\n    for i in range(n_filters, len(axes)):\\n        axes[i].axis(\\'off\\')\\n\\n    plt.suptitle(f\\'Filters of {layer.__class__.__name__} layer (in_channels={layer.in_channels}, out_channels={layer.out_channels})\\')\\n    plt.tight_layout()\\n    #plt.show()\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to train the model\n",
    "from datetime import datetime\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer,lr, epochs,keyboard_name,BUFFER):\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        epoch_losses = []\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            try:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                epoch_losses.append(loss.item())\n",
    "                \n",
    "                if i % 10 == 9:  # Print every 10 batches\n",
    "                    print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 10:.3f}\")\n",
    "                    running_loss = 0.0\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(\"Input tensors may not be of the right shape\", e)\n",
    "                raise e\n",
    "        \n",
    "        # Store average loss for this epoch\n",
    "        avg_epoch_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "        losses.append(avg_epoch_loss)\n",
    "        print(f\"Epoch {epoch + 1} average loss: {avg_epoch_loss:.3f}\")\n",
    "        \"\"\" Save model after each epoch\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_epoch_loss,\n",
    "        }, f\"{save_path}_epoch_{epoch+1}.pt\")\n",
    "        \"\"\"\n",
    "    filename  = datetime.now().strftime(\"%Y%m%d_%H%M%S\") + '_' + keyboard_name + '_keystroke_model.pt'\n",
    "    # Save final model\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': losses[-1],\n",
    "        'epochs': epochs, \n",
    "    }, filename)\n",
    "\n",
    "    plt.plot(range(epochs), losses)\n",
    "    plt.title(f'Training Loss for {epochs} epochs, {model} model, {criterion} loss function, and {lr} learning rate')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error')\n",
    "    plt.savefig(f'training_loss_{BUFFER}.png', dpi=300, bbox_inches='tight')\n",
    "    #plt.show()\n",
    "        \n",
    "    return losses, filename\n",
    "\n",
    "# Training call\n",
    "#losses, filename = train(model, train_loader, criterion, optimizer, 30,\"macbook\")\n",
    "\n",
    "#conv_layers = [layer for layer in model.modules() if isinstance(layer, nn.Conv2d)]\n",
    "\n",
    "\"\"\"\"\n",
    "for layer in conv_layers:\n",
    "    # Get weights for this specific layer\n",
    "    weights = layer.weight.data  # Shape: (out_channels, in_channels, kernel_size, kernel_size)\n",
    "    \n",
    "    # Calculate grid size based on number of output channels\n",
    "    n_filters = layer.out_channels\n",
    "    n_cols = 8  # Fixed number of columns\n",
    "    n_rows = (n_filters + n_cols - 1) // n_cols  # Calculate rows needed\n",
    "    \n",
    "    # Create figure with appropriate size\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 2*n_rows))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    # Plot each filter\n",
    "    for i in range(n_filters):\n",
    "        # Get the first channel of the filter (since we want to visualize 2D)\n",
    "        filter_weights = weights[i, 0].cpu().numpy()\n",
    "        \n",
    "        # Plot the filter\n",
    "        axes[i].imshow(filter_weights, cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f'Filter {i+1}')\n",
    "    \n",
    "    # Hide empty subplots if any\n",
    "    for i in range(n_filters, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Filters of {layer.__class__.__name__} layer (in_channels={layer.in_channels}, out_channels={layer.out_channels})')\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "\"\"\"\n",
    "\n",
    "# Plot training loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight: torch.Size([32, 1, 3, 3])\n",
      "conv1.bias: torch.Size([32])\n",
      "conv2.weight: torch.Size([64, 32, 3, 3])\n",
      "conv2.bias: torch.Size([64])\n",
      "conv3.weight: torch.Size([128, 64, 3, 3])\n",
      "conv3.bias: torch.Size([128])\n",
      "fc1.weight: torch.Size([512, 75776])\n",
      "fc1.bias: torch.Size([512])\n",
      "fc2.weight: torch.Size([26, 512])\n",
      "fc2.bias: torch.Size([26])\n",
      "Error: Could not extract key from filename: keystroke_67_Key.esc.npy. Make sure it is a valid key (aA-zZ)\n",
      "loaded 66 spectrograms\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "def load_model(filename):\n",
    "    if \"model\" not in locals() or type(model) != KeystrokeCNN:\n",
    "        print(\"Please load the model first\")\n",
    "        model = KeystrokeCNN()\n",
    "        if not filename or filename is None:\n",
    "            filename = input(\"Enter the filename of the saved model\")\n",
    "        checkpoint = torch.load(filename)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        if 'epochs' in checkpoint:\n",
    "            epochs = checkpoint['epochs']\n",
    "        else: \n",
    "            epochs = 100\n",
    "    \n",
    "    return model, epochs\n",
    "# Define character mapping (should match what you used in training)\n",
    "letters = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "idx2char = {i: c for i, c in enumerate(letters)}\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")\n",
    "\n",
    "def predict(model, spectrogram_tensor):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    with torch.no_grad():  # No need to compute gradients\n",
    "        output = model(spectrogram_tensor)  # Forward pass\n",
    "        probabilities = F.softmax(output, dim=1)  # Convert logits to probabilities\n",
    "        predicted_idx = torch.argmax(probabilities, dim=1).item()  # Get class index\n",
    "        predicted_letter = idx2char[predicted_idx]  # Convert index to letter\n",
    "        \n",
    "    return predicted_letter\n",
    "\n",
    "\n",
    "def crop_spectrogram(tensor, target_width=300):\n",
    "    return tensor[:, :, :target_width] \n",
    "\n",
    "\n",
    "#NUMPY_DIR = \"dell_test\"\n",
    "spectrogram_tensors, keys, max_width = load_spectrograms_from_directory(\"dell_test/numpy_arrays\")\n",
    "# torch.Size([1, 1, 129, *])\n",
    "\n",
    "\n",
    "def evaluate_model(model, spectrogram_tensors, keys,buffer):\n",
    "    y_hat = []\n",
    "    y_true = []\n",
    "    for i in range(len(spectrogram_tensors)):\n",
    "        test_tensor = spectrogram_tensors[i].unsqueeze(0)\n",
    "        predicted_letter = predict(model, test_tensor)\n",
    "        y_hat.append(predicted_letter)\n",
    "        y_true.append(keys[i])\n",
    "        #print(f\"Predicted letter: {predicted_letter}\")\n",
    "        #print(\"actual letter:\", keys[i])\n",
    "        letter_to_idx = {letter: idx for idx, letter in enumerate(letters)}\n",
    "        y_true_idx = [letter.item() for letter in y_true]\n",
    "        y_hat_idx = [letter_to_idx[letter] for letter in y_hat]\n",
    "\n",
    "        # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true_idx, y_hat_idx)\n",
    "    precision = precision_score(y_true_idx, y_hat_idx, average='weighted')\n",
    "    recall = recall_score(y_true_idx, y_hat_idx, average='weighted')\n",
    "    f1 = f1_score(y_true_idx, y_hat_idx, average='weighted')\n",
    "\n",
    "    # keys is a list of idx keys\n",
    "    \n",
    "    cm= confusion_matrix(y_hat, [letters[key] for key in keys])\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    classes = list(letters) \n",
    "    sns.heatmap(cm, \n",
    "                annot=True,  # Show numbers in each cell\n",
    "                fmt='d',     # Format as integers\n",
    "                cmap='Blues', # Color scheme\n",
    "                xticklabels=classes,  # Labels on x-axis: ['a', 'b', 'c', ..., 'z']\n",
    "                yticklabels=classes) \n",
    "    plt.title(f'Confusion Matrix for {10} epochs')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f'confusion_matrix_{buffer}.png', dpi=300, bbox_inches='tight')\n",
    "    #plt.show()\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "#print(evaluate_model(model, spectrogram_tensors, keys))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left for implementation : adding a live prediction \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "PortAudio library not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msounddevice\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msignal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msignal\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sounddevice.py:71\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 71\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPortAudio library not found\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     72\u001b[0m     _lib \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mdlopen(_libname)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "\u001b[0;31mOSError\u001b[0m: PortAudio library not found"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import scipy.signal as signal\n",
    "import torch\n",
    "from scipy.ndimage import zoom\n",
    "import time\n",
    "from queue import Queue\n",
    "import threading\n",
    "import librosa\n",
    "from scipy.signal import find_peaks\n",
    "import time\n",
    "\n",
    "letters = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "idx2char = {i: c for i, c in enumerate(letters)}\n",
    "\n",
    "class LiveKeystrokeDetector:\n",
    "    def __init__(self, model, sample_rate=44100, window_size=0.2, threshold=0.9999999):\n",
    "        self.model = model\n",
    "        self.sample_rate = sample_rate\n",
    "        self.window_size = window_size\n",
    "        self.threshold = threshold\n",
    "        self.window_samples = int(window_size * sample_rate)\n",
    "        self.audio_buffer = np.zeros(self.window_samples)\n",
    "        self.prediction_queue = Queue()\n",
    "        self.is_recording = False\n",
    "        self.debounce_time = time.time()\n",
    "        \n",
    "    def audio_callback(self, indata, frames, time_info, status):\n",
    "        \"\"\"Callback function for audio streaming\"\"\"\n",
    "        if status:\n",
    "            print(f\"Audio callback status: {status}\")\n",
    "            \n",
    "        # Update buffer with new audio data\n",
    "        self.audio_buffer = np.roll(self.audio_buffer, -frames)\n",
    "        self.audio_buffer[-frames:] = indata.flatten()\n",
    "        \n",
    "\n",
    "        mel_spect = librosa.feature.melspectrogram(\n",
    "            y=self.audio_buffer,\n",
    "            sr=self.sample_rate,\n",
    "            n_mels=80,           # Reduced from 128 - still detailed enough\n",
    "            n_fft=2048,          # Keep this - good balance\n",
    "            hop_length=512,      # Changed from 1024 for better temporal resolution\n",
    "            window='hann',       # Keep this - good choice\n",
    "            power=2.0            # Keep this - standard choice\n",
    "        )\n",
    "\n",
    "        # Convert to log scale (dB)\n",
    "        mel_spect_db = librosa.power_to_db(mel_spect, ref=np.max)\n",
    "\n",
    "        # Normalize to 0-1 range\n",
    "        mel_spect_norm = (mel_spect_db - mel_spect_db.min()) / (mel_spect_db.max() - mel_spect_db.min())\n",
    "\n",
    "        # Convert to tensor\n",
    "        spectrogram_tensor = torch.FloatTensor(mel_spect_norm)\n",
    "        \n",
    "        # Pad or truncate to 300 in the last dimension\n",
    "        current_length = spectrogram_tensor.shape[-1]\n",
    "        if current_length < 300:\n",
    "            # Pad to 300\n",
    "            padding = (0, 300 - current_length)\n",
    "            spectrogram_tensor = F.pad(spectrogram_tensor, padding, mode='constant', value=0)\n",
    "        elif current_length > 300:\n",
    "            # Center crop to 300\n",
    "            start = (current_length - 300) // 2\n",
    "            spectrogram_tensor = spectrogram_tensor[..., start:start+300]\n",
    "        \n",
    "        # Get the energy over time\n",
    "        energy = np.mean(mel_spect, axis=0)\n",
    "        \n",
    "        # Find peaks with minimum height and distance\n",
    "        peaks, _ = find_peaks(energy, \n",
    "                            height=0.5,          # Minimum height\n",
    "                            distance=20,         # Minimum samples between peaks\n",
    "                            prominence=0.3)      # Minimum prominence of peaks\n",
    "        \n",
    "        if len(peaks) > 0 and time.time() - self.debounce_time > 1:\n",
    "            print(\"Keystroke detected\")\n",
    "            # Make prediction\n",
    "            with torch.no_grad():\n",
    "                spectrogram_tensor = spectrogram_tensor.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions (1, 1, 80, 300)\n",
    "                output = model(spectrogram_tensor)  # Forward pass\n",
    "                probabilities = F.softmax(output, dim=1)  # Convert logits to probabilities\n",
    "                predictions = sorted([(idx2char[letter],prob.item()) for letter, prob in enumerate(probabilities[0])], key=lambda x: x[1], reverse=True)\n",
    "                print(predictions[:5])\n",
    "                predicted_idx = torch.argmax(probabilities, dim=1).item()  # Get class index\n",
    "                predicted_letter = idx2char[predicted_idx]  # Convert index to letter\n",
    "                self.prediction_queue.put(predicted_letter)\n",
    "\n",
    "            print(\"------------------------------------------\")\n",
    "            # add a delay to prevent multiple detections for the same keystroke\n",
    "            self.debounce_time = time.time()\n",
    "\n",
    "    \n",
    "    def start_recording(self):\n",
    "        \"\"\"Start recording and processing audio\"\"\"\n",
    "        self.is_recording = True\n",
    "        print(\"Starting live keystroke detection...\")\n",
    "        \n",
    "        try:\n",
    "            with sd.InputStream(samplerate=self.sample_rate, \n",
    "                              channels=1, \n",
    "                              callback=self.audio_callback):\n",
    "                while self.is_recording:\n",
    "                    # Process predictions from queue\n",
    "                    try:\n",
    "                        prediction = self.prediction_queue.get_nowait()\n",
    "                        print(f\"Predicted key: {prediction}\")\n",
    "                    except:\n",
    "                        pass\n",
    "                    time.sleep(0.01)  # Prevent high CPU usage\n",
    "                    \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nStopping recording...\")\n",
    "            self.is_recording = False\n",
    "    \n",
    "    def stop_recording(self):\n",
    "        \"\"\"Stop recording\"\"\"\n",
    "        self.is_recording = False\n",
    "\n",
    "    \n",
    "\n",
    "def main():\n",
    "    # Load your trained model\n",
    "    try: \n",
    "        model = KeystrokeCNN()\n",
    "        checkpoint = torch.load(filename)  # Replace with your model path\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"Model loaded successfully\")\n",
    "    except:\n",
    "        try: \n",
    "            print(\"Error: Could not load model\")\n",
    "            print(\"trying to load model from file\")\n",
    "            filename = \"20250324_222848_dell_keystroke_model.pt\" #input(\"enter filename of saved model\")\n",
    "            model = KeystrokeCNN()\n",
    "            checkpoint = torch.load(filename)  # Replace with your model path\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(\"Model loaded successfully\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Error: Could not load model\")\n",
    "            print(\"Please check the model file name and try again\")\n",
    "            return\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    # Create detector instance\n",
    "    detector = LiveKeystrokeDetector(model)\n",
    "    \n",
    "    try:\n",
    "        # Start recording\n",
    "        detector.start_recording()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nStopping...\")\n",
    "        detector.stop_recording()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #main()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra : optimizer for buffer time with analaysis of accuracy / precision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run([\"jupytext\", \"nbconvert\", \"--to\", \"py\", \"audio_sampling.ipynb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected device for training  :  cpu\n",
      "Error: Could not extract key from filename: keystroke_67_Key.esc.npy. Make sure it is a valid key (aA-zZ)\n",
      "loaded 66 spectrograms\n",
      "test keys ['t', 'j', 'g', 'd', 'v', 'k', 'h', 'g', 'e', 'h', 'b', 'c', 'r', 'e', 't', 'd', 'h', 'k', 'f', 'i', 'u', 'g', 'c', 'j', 'v', 'g', 'y', 'r', 'g', 'f', 'l', 'd', 'd', 'd', 'g', 't', 'r', 't', 'd', 'h', 'y', 'v', 'b', 'q', 'b', 'a', 's', 'j', 'u', 'o', 'f', 'e', 'a', 'g', 'h', 'i', 'f', 'y', 'e', 'g', 'g', 'd', 'f', 'g', 'o', 'd']\n",
      "test tensors 66\n",
      "Summary of the keystroke data : \n",
      "Total valid keystrokes: 1096\n",
      "Total invalid keystrokes: 11\n",
      "Keystroke times:\n",
      "Average keystroke duration:  0.10812529379562047\n",
      "Processing complete. Spectrograms and NumPy arrays saved.\n",
      "Error: Could not extract key from filename: keystroke_412_+.npy. Make sure it is a valid key (aA-zZ)\n",
      "Warning : Spectrogram tensor shape torch.Size([1, 8, 300]) does not match the expected size torch.Size([1, 129, 300])\n",
      "Error: Could not extract key from filename: keystroke_429_;.npy. Make sure it is a valid key (aA-zZ)\n",
      "Error: Could not extract key from filename: keystroke_205_Key.esc.npy. Make sure it is a valid key (aA-zZ)\n",
      "Error: Could not extract key from filename: keystroke_602_Key.space.npy. Make sure it is a valid key (aA-zZ)\n",
      "Warning : Spectrogram tensor shape torch.Size([1, 7, 300]) does not match the expected size torch.Size([1, 129, 300])\n",
      "Warning : Spectrogram tensor shape torch.Size([1, 8, 300]) does not match the expected size torch.Size([1, 129, 300])\n",
      "Error: Could not extract key from filename: keystroke_1096_Key.esc.npy. Make sure it is a valid key (aA-zZ)\n",
      "Warning : Spectrogram tensor shape torch.Size([1, 8, 300]) does not match the expected size torch.Size([1, 129, 300])\n",
      "loaded 1275 spectrograms\n",
      "Input shape: torch.Size([64, 1, 129, 300])\n",
      "Input shape: torch.Size([64, 1, 129, 300])\n",
      "Input shape: torch.Size([64, 1, 129, 300])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 87\u001b[39m\n\u001b[32m     84\u001b[39m         plt.savefig(\u001b[33m'\u001b[39m\u001b[33maccuracy_vs_buffer_time.png\u001b[39m\u001b[33m'\u001b[39m, dpi=\u001b[32m300\u001b[39m, bbox_inches=\u001b[33m'\u001b[39m\u001b[33mtight\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     85\u001b[39m         plt.show()\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtune_hyperparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mtune_hyperparameters\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     48\u001b[39m criterion = nn.CrossEntropyLoss()\n\u001b[32m     49\u001b[39m optimizer = optim.Adam(model.parameters(), lr)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m _, filename = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mBUFFER\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m generate_spectrograms(BUFFER,\u001b[33m\"\u001b[39m\u001b[33mdell_test\u001b[39m\u001b[33m\"\u001b[39m,extraction_method) \u001b[38;5;66;03m# resample the test file with the same buffer time as the training dataset\u001b[39;00m\n\u001b[32m     55\u001b[39m test_spectrogram_tensors, test_keys, max_width = load_spectrograms_from_directory(\u001b[33m\"\u001b[39m\u001b[33mdell_test/numpy_arrays\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;66;03m# now load the test set for evaluation\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, train_loader, criterion, optimizer, lr, epochs, keyboard_name, BUFFER)\u001b[39m\n\u001b[32m     13\u001b[39m optimizer.zero_grad()\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     loss = criterion(outputs, labels)\n\u001b[32m     18\u001b[39m     loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/MAIS202-FinalProject/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/MAIS202-FinalProject/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mKeystrokeCNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# print(\"After conv1+pool:\", x.shape)\u001b[39;00m\n\u001b[32m     39\u001b[39m  x = F.relu(\u001b[38;5;28mself\u001b[39m.conv2(x))\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m  x = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# print(\"After conv2+pool:\", x.shape)\u001b[39;00m\n\u001b[32m     43\u001b[39m  x = F.relu(\u001b[38;5;28mself\u001b[39m.conv3(x))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/MAIS202-FinalProject/venv/lib/python3.12/site-packages/torch/_jit_internal.py:624\u001b[39m, in \u001b[36mboolean_dispatch.<locals>.fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    622\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(*args, **kwargs)\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/MAIS202-FinalProject/venv/lib/python3.12/site-packages/torch/nn/functional.py:830\u001b[39m, in \u001b[36m_max_pool2d\u001b[39m\u001b[34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[39m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    829\u001b[39m     stride = torch.jit.annotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[32m--> \u001b[39m\u001b[32m830\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from audio_sampling import generate_spectrograms\n",
    "\n",
    "import csv \n",
    "\n",
    "def tune_hyperparameters():\n",
    "    \"\"\"\n",
    "    Tunes the main hyperparameters, notably \n",
    "\n",
    "    - frequency acquisition algorithm : fast Fourrier Transform or Mel spectrogram\n",
    "    - BUFFER timing\n",
    "    \"\"\"\n",
    "    BUFFER_TIME_MAX = 0.1\n",
    "    BUFFER_TIME_MIN = 0.0001\n",
    "    BUFFER_INCREMENT = 0.0001\n",
    "\n",
    "    NUMPY_DIR = \"dell/numpy_arrays\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(\"selected device for training  : \",device)\n",
    "\n",
    "\n",
    "    test_spectrogram_tensors, test_keys, max_width = load_spectrograms_from_directory(\"dell_test/numpy_arrays\") # now load the test set for evaluation\n",
    "\n",
    "    print(\"test keys\",test_keys)\n",
    "    print(\"test tensors\",len(test_spectrogram_tensors))\n",
    "\n",
    "    accuracies = []\n",
    "    for extraction_method in (\"FFT\",\"mel\"): \n",
    "        for BUFFER in np.arange(BUFFER_TIME_MIN, BUFFER_TIME_MAX, BUFFER_INCREMENT):\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            average_keystroke_duration = generate_spectrograms(BUFFER,\"dell\",extraction_method)\n",
    "            spectrogram_tensors, keys, max_width = load_spectrograms_from_directory(NUMPY_DIR)\n",
    "            \n",
    "            # Create label tensor for current dataset\n",
    "            letters = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "            char_to_idx = {c: i for i, c in enumerate(letters)}\n",
    "            label_tensor = torch.tensor([char_to_idx[k] for k in keys]).to(device)\n",
    "            \n",
    "            train_dataset = KeystrokeDataset(spectrogram_tensors, label_tensor)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "            \n",
    "            # Initialize model for each iteration\n",
    "            model = KeystrokeCNN(num_classes=len(letters)).to(device)\n",
    "            lr = 0.001\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr)\n",
    "            \n",
    "            _, filename = train(model, train_loader, criterion, optimizer,lr, 10,\"dell\",BUFFER)\n",
    "\n",
    "            generate_spectrograms(BUFFER,\"dell_test\",extraction_method) # resample the test file with the same buffer time as the training dataset\n",
    "\n",
    "            test_spectrogram_tensors, test_keys, max_width = load_spectrograms_from_directory(\"dell_test/numpy_arrays\") # now load the test set for evaluation\n",
    "\n",
    "            test_keys_tensors = torch.tensor([char_to_idx[k] for k in test_keys]).to(device)\n",
    "\n",
    "            results = evaluate_model(model, test_spectrogram_tensors, test_keys_tensors,BUFFER) # evaluate the model using test keys\n",
    "\n",
    "            accuracies.append((results[\"accuracy\"], BUFFER))\n",
    "            \n",
    "            print(f\"Buffer: {BUFFER:.6f}, Accuracy: {results['accuracy']:.4f}\")\n",
    "\n",
    "            with open(\"backup_results.txt\", \"a\") as f:\n",
    "                reader = csv.writer(f)\n",
    "                reader.writerow([BUFFER,results[\"accuracy\"],results[\"precision\"]])\n",
    "\n",
    "        # Separate accuracies and buffer times for plotting\n",
    "        accuracy_values = [acc for acc, _ in accuracies]\n",
    "        buffer_times = [buf for _, buf in accuracies]\n",
    "\n",
    "        with open(\"results.txt\", \"w\") as f:\n",
    "            for acc, buf in accuracies:\n",
    "                f.write(f\"Buffer: {buf:.6f}, Accuracy: {acc:.4f}\\n\")\n",
    "\n",
    "        # Create the plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(buffer_times, accuracy_values, 'b-', marker='o')\n",
    "        plt.xlabel('Buffer Time (seconds)')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Model Accuracy vs Buffer Time')\n",
    "        plt.grid(True)\n",
    "        plt.savefig('accuracy_vs_buffer_time.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "print(tune_hyperparameters())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
